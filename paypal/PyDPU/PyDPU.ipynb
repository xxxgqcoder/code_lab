{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f3a89a",
   "metadata": {},
   "source": [
    "# PyDPU documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341de9d",
   "metadata": {},
   "source": [
    "https://engineering.paypalcorp.com/confluence/display/RiskGRSDARTBO/MADMEn+Notebook+API+Tutorial#MADMEnNotebookAPITutorial-DriverFromHive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf7576",
   "metadata": {},
   "source": [
    "# get spark instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ad24e-29f3-42fa-a3a4-c1c402632ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from automation_utils.spark.session import get_spark\n",
    "\n",
    "spark_instance_num = 128\n",
    "\n",
    "spark_config = {\n",
    "    \"spark.driver.memoryOverhead\": \"8192\",\n",
    "    \"spark.executor.memoryOverhead\": \"8192\",\n",
    "    \"spark.executor.heartbeatInterval\": \"600s\",\n",
    "    \"spark.network.timeout\": \"2000s\",\n",
    "    \"spark.shuffle.service.enabled\": \"true\",\n",
    "    \"spark.sql.shuffle.partitions\": \"1000\",\n",
    "    \"spark.default.parallelism\": \"2400\",\n",
    "    \"spark.yarn.access.hadoopFileSystems\": \"hdfs://horton\",\n",
    "    \"spark.driver.maxResultSize\": \"32g\",\n",
    "    \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a662a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark(app_name='guxia_debug',\n",
    "                  queue='risk_gds_focus',\n",
    "                  executor_instances=spark_instance_num,\n",
    "                  executor_instances_min=spark_instance_num,\n",
    "                  executor_instances_max=spark_instance_num*2,\n",
    "                  executor_cores=1,\n",
    "                  driver_memory='16g',\n",
    "                  load_all_jars=True,\n",
    "                  auto_setup_ports=True,\n",
    "                  config=spark_config,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9259c-374c-4f61-ae59-eb6659f93277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark v2\n",
    "\n",
    "spark_config = {\n",
    "    \"spark.driver.memoryOverhead\": \"2048\",\n",
    "    \"spark.executor.memoryOverhead\": \"2048\",\n",
    "    \"spark.executor.heartbeatInterval\": \"1800s\",\n",
    "    \"spark.network.timeout\": \"3000s\",\n",
    "    \"spark.shuffle.service.enabled\": \"false\",\n",
    "    \"spark.executor.instances\": \"500\",\n",
    "    \"spark.sql.shuffle.partitions\": \"2000\",\n",
    "    \"spark.default.parallelism\": \"3000\",\n",
    "    \"spark.yarn.access.hadoopFileSystems\": \"hdfs://horton\",\n",
    "    \"spark.driver.maxResultSize\": \"8g\",\n",
    "    \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "}\n",
    "\n",
    "spark = get_spark(\n",
    "    app_name=f\"data_process\",\n",
    "    queue='risk_gds_focus',\n",
    "    executor_memory='30g',\n",
    "    dynamic_allocation=False,\n",
    "    executor_instances_max=1000,\n",
    "    driver_cores=\"4\",\n",
    "    config=spark_config,\n",
    "    load_all_jars=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71e6f7",
   "metadata": {},
   "source": [
    "# use ModelResultReader to pull model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ed415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_dpu import ModelResultReader\n",
    "from py_dpu import save_pig\n",
    "from model_automation.utils.rmr import run_cmd\n",
    "\n",
    "\n",
    "\n",
    "data_hdfs_dir = ''\n",
    "\n",
    "meta_cols = \"\"\"\n",
    "jms_queue_name\n",
    "jms_message_name\n",
    "message_name\n",
    "message_id\n",
    "message_type\n",
    "service_name\n",
    "time_published\n",
    "time_consumed\n",
    "current_page\n",
    "total_pages\n",
    "app_metadata\n",
    "event_date\n",
    "event_time\n",
    "ATOM16Seg\n",
    "AW_ANALYZER_VERSION\n",
    "AW_MODEL_NAME\n",
    "AW_UUID\n",
    "BREAllInOneTrack\n",
    "IS_AUDIT_SKIPPED\n",
    "IS_AUDIT_TRACK\n",
    "RUCS_UNIQUE_ID\n",
    "SellerSeg\n",
    "account_number\n",
    "activity_id\n",
    "isDCC\n",
    "is_ucc\n",
    "corr_id_\n",
    "\"\"\".strip().split()\n",
    "\n",
    "variables = \"\"\"\n",
    "\"\"\".strip().split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ba562-4153-4105-a98f-e23cd5242870",
   "metadata": {},
   "source": [
    "- usually we run BREAllInOneTrack for compute item.\n",
    "- set a smaller time window, or it may take too long time to finish\n",
    "- use sample option is faster than limit option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89454b0-de3e-4def-8ba1-80924e2bf05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark instance\n",
    "spark = None\n",
    "\n",
    "try:\n",
    "    run_cmd(f\"hadoop fs -rm -skipTrash {data_hdfs_dir}/*\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "mreader = ModelResultReader(spark,\n",
    "                            checkpoint='ConsolidatedFunding',\n",
    "                            computeItem='BREAllInOneTrack')\\\n",
    "                                .timeFilter('2023-05-07 00:00:00 to 2023-05-07 01:00:00')\\\n",
    "                                .option('sample', '0.01')\n",
    "\n",
    "ret_df = mreader.load()\n",
    "\n",
    "audit_data = ret_df.select(meta_cols + variables)\n",
    "\n",
    "audit_data.printSchema()\n",
    "\n",
    "audit_data.coalesce(4).write.mode('overwrite').parquet(data_hdfs_dir)\n",
    "\n",
    "print(f'data saved to {data_hdfs_dir}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843d677",
   "metadata": {},
   "source": [
    "# pull madmen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23851d",
   "metadata": {},
   "source": [
    "- need to delete outputdir, or madmen will load previous data instead of generating a new for you.\n",
    "- the API is running an inner join way, so you can check record number after pulling variable to see coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from py_dpu import loadByDriver\n",
    "from py_dpu import HdfsManager\n",
    "from py_dpu import estimate_partition_num\n",
    "\n",
    "\n",
    "def madmen_pull_variable(spark,\n",
    "                         variables,\n",
    "                         output_dir,\n",
    "                         driver_df,\n",
    "                         checkpoint,\n",
    "                         driver_start_dt,\n",
    "                         driver_end_dt,\n",
    "                         driver_date_col,\n",
    "                         driver_key,\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    pull madmen data using a driver.\n",
    "    :param spark: spark instance.\n",
    "    :param variables: variables you want to pull.\n",
    "    :param output_dir: output data directory. This code will output one folder for each month in driver under output_dir,\n",
    "                        and one folder of same data in parquet format.\n",
    "    :param driver_df: driver dataframe.\n",
    "    :param checkpoint: checkpoint name.\n",
    "    :param driver_start_dt: start date of data range in driver.\n",
    "    :param driver_end_dt: end date of data range in driver.\n",
    "    :param driver_date_col: date column name in driver.\n",
    "    :param driver_key: key in driver to pull madmen, usually it's txn id.\n",
    "    \"\"\"\n",
    "\n",
    "    start_dt = datetime.strptime(driver_start_dt, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(driver_end_dt, '%Y-%m-%d')\n",
    "    \n",
    "    num_day = (end_dt - start_dt).days + 1\n",
    "    filter_expr = f\"{driver_date_col} between '{driver_start_dt}' and '{driver_end_dt}'\"\n",
    "    print(f'filter expression: {filter_expr}')\n",
    "    data = driver_df.filter(filter_expr)\n",
    "    rec_num = data.count()\n",
    "    print(f'filtered data num: {rec_num}')\n",
    "\n",
    "    output_data_dir = f\"{output_dir}/{start_dt.strftime('%Y%m%d')}_{end_dt.strftime('%Y%m%d')}\"\n",
    "    output_parquet_dir = f\"{output_dir}/{start_dt.strftime('%Y%m%d')}_{end_dt.strftime('%Y%m%d')}_parquet\"\n",
    "    \n",
    "    hdfs = HdfsManager(spark)\n",
    "    hdfs.delete(path=output_data_dir)\n",
    "    \n",
    "    print(f'output dir: {output_parquet_dir}')\n",
    "\n",
    "    time_window = f\"{driver_start_dt} to {driver_end_dt}\"\n",
    "    print(f\"time widnow: {time_window}\")\n",
    "    \n",
    "    df = loadByDriver(spark,\n",
    "                      checkpoint=checkpoint,\n",
    "                      driverSet=data,\n",
    "                      time=time_window,\n",
    "                      variables=variables,\n",
    "                      dateColumn=driver_date_col,\n",
    "                      driverKeys=[driver_key],\n",
    "                      madmenKeys=['transaction'],\n",
    "                      outputPath=output_data_dir,\n",
    "                      groupSize=num_day)\n",
    "    \n",
    "    print('pulled rec num')\n",
    "    rec_num = df.groupBy(driver_date_col).count().sort(driver_date_col)\n",
    "    rec_num.show(num_day)\n",
    "    \n",
    "    partition_num = estimate_partition_num(spark, output_data_dir, 1024)\n",
    "    df.coalesce(partition_num).write.mode('overwrite').parquet(output_parquet_dir)\n",
    "    print(f'data saved to {output_parquet_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844fb37",
   "metadata": {},
   "source": [
    "# move data from Teradata to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d2d33-9d9a-494c-98f8-6c5e9db662dd",
   "metadata": {},
   "source": [
    "-  NOTE: for string type, loaded columns data contains extra space, trim it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43833b1e-69db-424d-8015-82640e76b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_dpu import save_pig\n",
    "from py_dpu.teradata import TeradataManager\n",
    "from credentials import TD_USER, TD_PASSWORD\n",
    "\n",
    "spark = None # spark instance\n",
    "td_mgr = TeradataManager(spark, TD_USER, TD_PASSWORD)\n",
    "1\n",
    "\n",
    "def move_to_hdfs(src_table, dt_column, dt_range, target_hdfs_dir):\n",
    "    from pyspark.sql.functions import col, trim\n",
    "    from pyspark.sql.types import StringType\n",
    "    \n",
    "    print(f'moving data from {src_table} to {target_hdfs_dir}, dt range: {dt_range}')\n",
    "    \n",
    "    df = td_mgr.load_by_date(\n",
    "        time=dt_range,\n",
    "        table=src_table,\n",
    "        pit_column=dt_column,\n",
    "    )\n",
    "    \n",
    "    for c in df.schema.fields:\n",
    "        if not isinstance(c.dataType, StringType):\n",
    "            continue\n",
    "        df = df.withColumn(c.name, trim(col(c.name)))\n",
    "    \n",
    "    save_pig(spark, df.coalesce(8), target_hdfs_dir, delimiter='\\x07')\n",
    "    print(f'finish saving driver to {target_hdfs_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adec8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55d701af-35df-4245-9ffb-ecd03a24b75a",
   "metadata": {},
   "source": [
    "# get data from hdfs to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db274ed9-4840-4e5c-a266-a0c08e22f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from automation_utils.common.hdfs import get_hdfs_to_local_csv\n",
    "\n",
    "hdfs_data_dir = 'hdfs://horton/apps/risk/det/guxia/ACDC_COMPONENT/task/addr_pattern/decline_tagging_20230717'\n",
    "local_data_path = 'data/decline_tagging_20230717/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282f542-9bf0-4c71-81ac-67942686cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_dir = os.path.dirname(local_data_path)\n",
    "os.makedirs(local_data_dir, exist_ok=True)\n",
    "print(f'downloading data from {hdfs_data_dir}')\n",
    "\n",
    "get_hdfs_to_local_csv(hdfs_data_dir,\n",
    "                      local_data_path,\n",
    "                      os.path.join(hdfs_data_dir, '.pig_header'))\n",
    "print(f'data downloaded to {local_data_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f0b64-7567-45cb-a0b9-86df3cf06efd",
   "metadata": {},
   "source": [
    "# upload local data to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc4b96-75a0-4281-9d58-e64c38906ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from automation_utils.common.hdfs import get_hdfs_to_local_csv, upload_local_file_to_hdfs\n",
    "\n",
    "local_data_path = ''\n",
    "hdfs_data_dir = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011b47f-93e0-4e31-ac0f-56adf9c613ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_local_file_to_hdfs(\n",
    "    local_file=local_data_path,\n",
    "    hdfs_path=hdfs_data_dir\n",
    ")\n",
    "print(f'finish upload data from {local_data_path} to {hdfs_data_dir}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870eac68-7cfd-4f25-a5c9-77961695ba67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2227ec91-5258-485e-829f-23f4c954bd10",
   "metadata": {},
   "source": [
    "# pull context data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69df53-cc44-4aad-ae89-0b26dc33a4e3",
   "metadata": {},
   "source": [
    "\n",
    "reference\n",
    "\n",
    "- https://github.paypal.com/DART/eve-builder-util/blob/main/bizlog-pull/src/main/resources/pull-context-bizlog-example.ipynb\n",
    "\n",
    "- can found context schema path from citadel: go/citadel\n",
    "\n",
    "- request.header.2 format: `correlation-id:5c92069e8250b`\n",
    "- can use this schema to extract k-v pair schema: `\"findInKVPairs(request.body.context.data_set.k_v_pairs, 'ucc_sender_email_address')\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefac3c-f0ee-45f5-b497-9d4b316c84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bizlog_pull_py.bom_bizlog_tool import BomBizlogTool\n",
    "from bizlog_pull_py.event_log_tool import EventLogTool\n",
    "from bizlog_pull_py.context_log_tool import ContextLogTool\n",
    "\n",
    "\n",
    "context_data_dir = '/sys/pp_dm/dm_hdp_batch/kafka_data/RISK/BLOGGING/riskunifiedcomputeserv/SC_riskunifiedcomputeserv/2023/11/01/12/*'\n",
    "\n",
    "data_save_dir = 'hdfs://horton/apps/risk/det/guxia/UCC23_V2/task/context_variable/context_2023110112'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd9475-747a-40f2-afb8-b9208de6e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = None\n",
    "\n",
    "contextTool = ContextLogTool(spark)\n",
    "\n",
    "contextTool.contextPath(context_data_dir)\n",
    "contextTool.filter(\"request.body.context.caller_info.flow_name == 'DCC'\n",
    "contextTool.elements(\n",
    "    'request.header.2', # correlation id\n",
    "    'request.body.context.client_configuration.api',\n",
    "    'request.body.context.client_configuration.integration_artifact',\n",
    "    'request.body.context.caller_info.flow_name',\n",
    ")\n",
    "\n",
    "\n",
    "df = contextTool.pull()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac1a29-c066-475c-ab68-7409e4abe436",
   "metadata": {},
   "source": [
    "# move data between GCP & HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f7809-c13f-4010-80fa-37855e45ab7b",
   "metadata": {},
   "source": [
    "notes:\n",
    "- will not overwrite dst data dir if file from src is found in dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6fb542-cf20-4145-a0c8-b64c08aed1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_automation.utils.gcp_utils.data_manipulate import move_data_gcp_hadoop\n",
    "from model_automation.utils.rmr import run_cmd\n",
    "\n",
    "\n",
    "src_data_dir = ''\n",
    "dst_data_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75b787d-981d-4fa3-b196-19d2bf7dc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# rm target folder\n",
    "# try:\n",
    "#     run_cmd(f'gsutil -m rm -r {dst_data_dir}')\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     pass\n",
    "    \n",
    "try:\n",
    "    run_cmd(f'hadoop fs -rm -r -skipTrash {dst_data_dir}')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "run_cmd(f'hadoop fs -mkdir -p {dst_data_dir}')\n",
    "\n",
    "move_data_gcp_hadoop(\n",
    "    src=src_data_dir,\n",
    "    dest=dst_data_dir,\n",
    "    show_log=True,\n",
    "    check=False\n",
    ")\n",
    "\n",
    "print(f'finish moving data from {src_data_dir} to {dst_data_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9bfbd-e1d1-400b-941e-4edb00b0242b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75a37762-c182-4739-9520-5ed2dfb15b64",
   "metadata": {},
   "source": [
    "# get columns from pig header file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05570485-b0fa-4333-a029-8486a1a3bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_automation.utils.rmr import run_cmd\n",
    "\n",
    "\n",
    "dev_data_dir = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65cb236-58d7-4d98-b760-2b3df7a5c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ret = run_cmd(f\"hadoop fs -cat {os.path.join(dev_data_dir, '.pig_header')}\", print_ret=False, get_ret=True)\n",
    "columns = ret.split('\\x07')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
