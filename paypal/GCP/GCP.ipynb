{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e27914-2df8-47a1-9511-71fcec408e71",
   "metadata": {},
   "source": [
    "# data movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01700001",
   "metadata": {},
   "source": [
    "## copy from HDFS to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004e3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# 2023-11-02: deprecated\n",
    "\n",
    "cd $(dirname \"$0\")\n",
    "\n",
    "echo $(pwd)\n",
    "\n",
    "sudo -Eu dt_hdp_ops /opt/conda/bin/kinit -kt /etl/LVS/dt_etldata6/dt_hdp_ops/.keytabs/dt_hdp_ops.keytab dt_hdp_ops\n",
    "\n",
    "source_dir=''\n",
    "target_dir=''\n",
    "\n",
    "sudo -Eu dt_hdp_ops \\\n",
    "    /etl/LVS/dt_etldata6/dt_hdp_ops/distcp/chadoop distcp \\\n",
    "    -Dmapreduce.map.java.opts=-Xmx2048m \\\n",
    "    -Dmapreduce.map.memory.mb=2048 \\\n",
    "    -overwrite \\\n",
    "    -delete \\\n",
    "    ${source_dir} ${target_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61880e05",
   "metadata": {},
   "source": [
    "## V2: use `maglev` magic in maglev kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_dir = '' # source data dir\n",
    "target_data_dir = '' # target data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%maglev copy $source_data_dir $target_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ea1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b8d80-b4a7-446b-b02b-7b6b1742e4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d37c49-6863-44e1-9ecd-91e837c97729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3d7fdf3",
   "metadata": {},
   "source": [
    "# submit job to GCP cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688db91-c440-4692-ab69-1f45ae370472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aml.cloud_v1 as cloud\n",
    "from model_automation.gcp import dataproc_config\n",
    "\n",
    "import job_funcs\n",
    "importlib.reload(job_funcs)\n",
    "from job_funcs import data_func\n",
    "\n",
    "\n",
    "gcp_project_id = 'ccg24-hrzana-gds-focus'\n",
    "gcp_client = cloud.DataProcClient(gcp_project_id)\n",
    "\n",
    "bq_project = 'pypl-bods'\n",
    "bq_dataset = 'rsh_row_gds_focus'\n",
    "packages_to_install = ['automation_utils==0.3.0', 'gcsfs']\n",
    "\n",
    "# object here must be json serializable\n",
    "params = {\n",
    "    'gcs_data_dir': 'gs://pypl-bkt-rsh-row-std-gds-focus/user/guxia/ucc_latam/cc_trust/simulation',\n",
    "    'bq_project': bq_project,\n",
    "    'bq_dataset': bq_dataset,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a1bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# create a one-time cluster and submit a job\n",
    "# submit job\n",
    "job_id = gcp_client.create_spark_job (\n",
    "    # function run on dataproc\n",
    "    func=data_move_job,\n",
    "    packages_to_install = packages_to_install,\n",
    "    pyfiles_to_import=['job_funcs.py'],\n",
    "    custom_billing_tag='guxia',\n",
    "    # function kwargs\n",
    "    params=params,\n",
    "    # dataproc config\n",
    "    **dataproc_config['bq_only'],\n",
    ")\n",
    "\n",
    "# wait until job finished\n",
    "status = gcp_client.wait_job_for_completion(job_id)\n",
    "if status == \"FAILED\":\n",
    "    print(f\"job failed, job id: {job_id}\")\n",
    "\n",
    "# save job log to local\n",
    "log_name = job_id.split(os.sep)[-1]\n",
    "os.makedirs('log', exist_ok=True)\n",
    "with open(os.path.join('log', log_name), 'w') as f:\n",
    "    gcp_client.get_job_logs(job_id, file=f)\n",
    "\n",
    "print(f'finish running data job')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a55ec-4c88-42cc-a2c6-50fb4fc49f07",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86633d80-50e4-4297-b8db-3835ec702409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6049c-5a6a-4b96-b053-3e4d14263832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af01b7-6f03-4fb2-aeaa-096f9e328f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c3c85c9-3b9c-439b-b40b-dec61a00df95",
   "metadata": {},
   "source": [
    "# move driver from BQ to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93535f-f156-44a8-b1cb-2156d484de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aml.cloud_v1 as cloud\n",
    "from model_automation.gcp import dataproc_config\n",
    "\n",
    "\n",
    "run_dt = datetime(2023, 11, 1)\n",
    "print('running dt', run_dt)\n",
    "\n",
    "driver_start_dt = datetime(2023, 7, 1)\n",
    "driver_end_dt = datetime(2023, 7, 2)\n",
    "print(driver_end_dt)\n",
    "\n",
    "bq_driver_table = 'pypl-bods.rsh_row_gds_focus.ucc_rmr_driver_sampled'\n",
    "driver_dt_column = 'pmt_start_date'\n",
    "gcs_driver_dir = f\"gs://pypl-bkt-rsh-row-std-gds-focus/user/guxia/UCC23_V2/data/driver/{run_dt.strftime('%Y%m%d')}/{driver_start_dt.strftime('%Y%m%d')}_{driver_end_dt.strftime('%Y%m%d')}\"\n",
    "print(gcs_driver_dir)\n",
    "\n",
    "gcp_project_id = 'ccg24-hrzana-gds-focus'\n",
    "bq_project = 'pypl-bods'\n",
    "bq_dataset = 'rsh_row_gds_focus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa87b3-7aa1-486f-add5-8e1a50a62375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_move_job(params):\n",
    "    import sys\n",
    "    sys.path.append(\"dpu-latest.jar\")\n",
    "    from datetime import datetime\n",
    "\n",
    "    from pyspark.sql.functions import col, to_date\n",
    "    \n",
    "    from py_dpu import BigqueryManager, HdfsManager\n",
    "    from py_dpu import load_pig, save_pig\n",
    "    \n",
    "    driver_start_dt = params.get('driver_start_dt', None)\n",
    "    driver_end_dt = params.get('driver_end_dt', None)\n",
    "    bq_driver_table = params.get('bq_driver_table', None)\n",
    "    gcs_driver_dir = params.get('gcs_driver_dir', None)\n",
    "    bq_project = params.get('bq_project', None)\n",
    "    bq_dataset = params.get('bq_dataset', None)\n",
    "    driver_dt_column = params.get('driver_dt_column', None)\n",
    "    \n",
    "    print('driver start dt', driver_start_dt)\n",
    "    print('driver end dt', driver_end_dt)\n",
    "    \n",
    "    # write your job code here\n",
    "    query = f\"\"\"\n",
    "        select *\n",
    "        from {bq_driver_table}\n",
    "        where\n",
    "            {driver_dt_column} between '{driver_start_dt}' and '{driver_end_dt}'\n",
    "    \"\"\"\n",
    "    print('query to run', query)\n",
    "\n",
    "    bq_mgr = BigqueryManager(spark)\n",
    "    hdfs_mgr = HdfsManager(spark)\n",
    "\n",
    "    tmp_gcs_driver_dir = gcs_driver_dir + '_tmp'\n",
    "    hdfs_mgr.delete(tmp_gcs_driver_dir)\n",
    "    bq_mgr.store_query_to_hdfs(query, project=bq_project, dataset=bq_dataset, output_path=tmp_gcs_driver_dir)\n",
    "    print(f'data saved to {tmp_gcs_driver_dir}')\n",
    "    \n",
    "    driver_df = spark.read.options(delimiter='\\x07', header=True).csv(f\"{tmp_gcs_driver_dir}/*csv\")\n",
    "\n",
    "    print('driver daily rec num')\n",
    "    driver_df.groupby(driver_dt_column).count().sort(col(driver_dt_column)).show(1000)\n",
    "\n",
    "    save_pig(spark, driver_df.coalesce(16), gcs_driver_dir, delimiter='\\x07')\n",
    "    print(f'data saved to {gcs_driver_dir}')\n",
    "\n",
    "    hdfs_mgr.delete(tmp_gcs_driver_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e72aa-cb4f-46da-adac-a5697342882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gcp_client = cloud.DataProcClient(gcp_project_id)\n",
    "\n",
    "# object here must be json serializable\n",
    "params = {\n",
    "    'driver_start_dt': driver_start_dt.strftime('%Y-%m-%d'),\n",
    "    'driver_end_dt': driver_end_dt.strftime('%Y-%m-%d'),\n",
    "    'bq_driver_table': bq_driver_table,\n",
    "    'gcs_driver_dir': gcs_driver_dir,\n",
    "    'driver_dt_column': driver_dt_column,\n",
    "    'bq_project': bq_project,\n",
    "    'bq_dataset': bq_dataset,\n",
    "}\n",
    "\n",
    "print('job params', json.dumps(params, indent=4))\n",
    "\n",
    "packages_to_install = ['automation_utils==0.3.0', 'gcsfs']\n",
    "\n",
    "# submit job\n",
    "job_id = gcp_client.create_spark_job (\n",
    "    # function run on dataproc\n",
    "    func=data_move_job,\n",
    "    packages_to_install = packages_to_install,\n",
    "    pyfiles_to_import=[],\n",
    "    custom_billing_tag='guxia',\n",
    "    # function kwargs\n",
    "    params=params,\n",
    "    # dataproc config\n",
    "    **dataproc_config['bq_only'],\n",
    ")\n",
    "\n",
    "# wait until job finished\n",
    "status = gcp_client.wait_job_for_completion(job_id)\n",
    "if status == \"FAILED\":\n",
    "    print(f\"job failed, job id: {job_id}\")\n",
    "\n",
    "# save job log to local\n",
    "log_name = job_id.split(os.sep)[-1]\n",
    "os.makedirs('log', exist_ok=True)\n",
    "with open(os.path.join('log', log_name), 'w') as f:\n",
    "    gcp_client.get_job_logs(job_id, file=f)\n",
    "\n",
    "print(f'finish running data movement job')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b07cd-49ee-408b-863b-28208f17fbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
