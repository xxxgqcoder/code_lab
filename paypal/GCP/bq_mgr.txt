class BigqueryManager(builtins.object)
  A big query manager class, mainly used to access BQ
  Main features:
  1. run BQ sql
  2. run BQ sql with params(default available params: starDate & endDate)
  3. load BQ table to DataFrame
  4. load BQ tables and store to Hadoop path, default format is csv with  seperated.
  5. store DataFrame into Big query table
  
  Methods defined here:
  
  __init__(self, spark)
      generate target sql based on sql template, time window and runtime params
      :param spark: sparkSession instance
      :return:
  
  drop_temp_tables(self, time, sql, params={}, parallel=2)
      drop the temp tables generated by the parallel mode.
          Required the same time & sql & params & parallel when generated.
      :param time: Time Filter in below format
              yyyy/MM/dd, yyyy-MM-dd, yyyy/MM/dd/HH, yyyy-MM-dd HH:mm:ss,
              yyyy-MM-dd to yyyy-MM-dd, yyyy/MM/dd/HH to yyyy/MM/dd/HH,
              yyyy-MM-dd HH:mm:ss to yyyy-MM-dd HH:mm:ss
              eg. "2019-09-28 00:00:00 to 2019-09-28 01:00:00" or "2019-09-28"
      :param sql: sql template string
      :param params: parameter dict for sql template.
      :param parallel: how many groups run in parallel. Default is 2.
      :return:
  
  gen_sql(self, time, sql, params={})
      generate target sql based on sql template, time window and runtime params
      :param time: Time Filter in below format
              yyyy/MM/dd, yyyy-MM-dd, yyyy/MM/dd/HH, yyyy-MM-dd HH:mm:ss,
              yyyy-MM-dd to yyyy-MM-dd, yyyy/MM/dd/HH to yyyy/MM/dd/HH,
              yyyy-MM-dd HH:mm:ss to yyyy-MM-dd HH:mm:ss
              eg. "2019-09-28 00:00:00 to 2019-09-28 01:00:00" or "2019-09-28"
      :param sql: sql template string
      :param params: parameter dict for sql template.
      :return: parsed sql string
  
  generate_sql_parallel(self, time, sql, params={}, parallel=2)
      generate runtime sql with dict returned in parallel. Dict( 'version' -> runtime sql)
      :param time: Time Filter in below format
              yyyy/MM/dd, yyyy-MM-dd, yyyy/MM/dd/HH, yyyy-MM-dd HH:mm:ss,
              yyyy-MM-dd to yyyy-MM-dd, yyyy/MM/dd/HH to yyyy/MM/dd/HH,
              yyyy-MM-dd HH:mm:ss to yyyy-MM-dd HH:mm:ss
              eg. "2019-09-28 00:00:00 to 2019-09-28 01:00:00" or "2019-09-28"
      :param sql: sql template string
      :param params: parameter dict for sql template.
      :param parallel: how many groups run in parallel. Default is 2.
      :return: Dict
  
  load(self, table)
      load BQ table to spark DataFrame
      Please note that: external tables are not supported.
      :param table: target table name in BQ
      :return: DataFrame
  
  run(self, sql)
      run sql in BQ
      :param sql: sql string
      :return:
  
  run_in_parallel(self, time, sql, params={}, final_tables=[], output_path=None, parallel=3, group_num=3, auto_clean=True)
      parallel run sql with params in Big Query(will auto skip success group).
      Final output path for each final table will be below:
          $output_path/$table_name/$version (csv, delimiter=\u0007, header=true)
          eg:
              gs://bucket/base_path/final_table/_ver_20210320_20210322
      Optional:
          store the final_tables into hadoop path and clean the temp tables in BQ.
      :param time: Time Filter in below format (Can't be empty) :
              yyyy/MM/dd, yyyy-MM-dd, yyyy/MM/dd/HH, yyyy-MM-dd HH:mm:ss,
              yyyy-MM-dd to yyyy-MM-dd, yyyy/MM/dd/HH to yyyy/MM/dd/HH,
              yyyy-MM-dd HH:mm:ss to yyyy-MM-dd HH:mm:ss
              eg. "2019-09-28 00:00:00 to 2019-09-28 01:00:00" or "2019-09-28"
      :param sql: sql template string
      :param params: parameter dict for sql template.
      :param final_tables: target table list in BQ.
      :param output_path: output path in Hadoop, default format is csv with  seperated.
      :param parallel: control the threads counts that running in the same time.
      :param auto_clean: clean the temp tables in BQ or not. Default is True.
      :param group_num: Split the sql into ${group_num} parts based on the time window and run in parallel.
          eg: time window is 2021-06-01 to 2021-08-31
              recommended groupSize value can be set into 3
      :return: Reload DataFrame from output path. Only have value when len(final_tables) = 1.
  
  run_with_param(self, sql, params={}, time=None, final_tables=[], output_path=None, auto_clean=True)
      run sql with params in BQ.
      Optional:
          store the final_tables into hadoop path and clean the temp tables in BQ.
      Possible Hadoop output path(csv, delimiter=, header=true):
          if sql template used version param:
              $output_path/$table_name/$version
          else:
              $output_path/$table_name
      :param time: Time Filter in below format (can be None) :
              yyyy/MM/dd, yyyy-MM-dd, yyyy/MM/dd/HH, yyyy-MM-dd HH:mm:ss,
              yyyy-MM-dd to yyyy-MM-dd, yyyy/MM/dd/HH to yyyy/MM/dd/HH,
              yyyy-MM-dd HH:mm:ss to yyyy-MM-dd HH:mm:ss
              eg. "2019-09-28 00:00:00 to 2019-09-28 01:00:00" or "2019-09-28"
      :param sql: sql template string
      :param params: parameter dict for sql template.
      :param final_tables: target table list in BQ. Can be empty(only run the sql with param)
      :param output_path: output path in Hadoop. Can be empty(only run the sql with param)
      :param auto_clean: clean the temp tables in BQ or not(after store the data into hadoop).
              Default is True.
              Only works when output path is valid.
      :return: Reload DataFrame from output path. Only have value when len(final_tables) = 1.
  
  store_query_to_hdfs(self, query, project, dataset, output_path)
      load BQ query to spark DataFrame and store to Hadoop path. Default output format is csv with delimiter "\u0007"
      :param query: query sql
      :param project: project which can store the query
      :param dataset: dataset which can store the query
      :param output_path: hdfs/gcs path
      :return:
  
  store_to_bq(self, df, table, mode='overwrite', options={})
      Save spark dataframe to biq query table.
      :param df: spark dataframe
      :param table: table name
      :param mode: overwrite or append
      :param options: a dict to specify more write options.
          https://github.com/GoogleCloudDataproc/spark-bigquery-connector#properties
      :return:
  
  store_to_hdfs(self, tables=[], output_path='', version='')
      load BQ tables to spark DataFrame and store to Hadoop path. Default output format is csv with delimiter "\u0007"
      Please note that: external tables are not supported.
      :param tables: target table list in BQ
      :param output_path: output path in hadoop
      :param version: unique version suffix for tables. Default is "".
              Note that: the final target tables will be $table$version
      :return:
  
  ----------------------------------------------------------------------
  Data descriptors defined here:
  
  __dict__
      dictionary for instance variables (if defined)
  
  __weakref__
      list of weak references to the object (if defined)
