{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa23de72-81d1-4eae-87f2-b0ef2bc60a6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-26T12:58:14.223761Z",
     "iopub.status.busy": "2023-08-26T12:58:14.223488Z",
     "iopub.status.idle": "2023-08-26T12:58:18.696623Z",
     "shell.execute_reply": "2023-08-26T12:58:18.696161Z",
     "shell.execute_reply.started": "2023-08-26T12:58:14.223737Z"
    }
   },
   "source": [
    "pulling madmen variable in GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dce9aa-7595-4c6c-9247-3dbe40cc9135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pull_madmen_monthly(params):\n",
    "    import sys\n",
    "    from datetime import datetime\n",
    "    sys.path.append(\"dpu-latest.jar\")\n",
    "\n",
    "    start_ts = datetime.now()\n",
    "\n",
    "    from pyspark.sql.functions import udf, col\n",
    "    from pyspark.sql.types import StringType\n",
    "\n",
    "    from py_dpu import load_pig, load_parquet, save_pig, rename_df, loadByDriver, HdfsManager, Statistics\n",
    "    from automation_utils.common.mail import send_email\n",
    "\n",
    "    var_list = params.get(\"var_list\")\n",
    "    driver_date_column = params.get('driver_date_column')\n",
    "    driver_monthly_column = params.get('driver_monthly_column')\n",
    "    checkpoint = params.get('checkpoint')\n",
    "    madmen_time_range = params.get('madmen_time_range')\n",
    "    email_to = params.get(\"email_to\")\n",
    "    group_size = params.get(\"group_size\", -1)\n",
    "    driver_gcs_dir = params.get(\"driver_gcs_dir\")\n",
    "    madmen_gcs_dir = params.get(\"madmen_gcs_dir\")\n",
    "    monthly = params.get(\"monthly\")\n",
    "\n",
    "    @udf(returnType=StringType())\n",
    "    def clean_trans_id(trans_id):\n",
    "        return trans_id.split('.')[0]\n",
    "\n",
    "    driver_df = load_pig(spark, driver_gcs_dir)\n",
    "    driver_set = rename_df(spark, driver_df, prefix='driver_')\n",
    "    driver_set = driver_set.withColumn('driver_trans_id_clean', clean_trans_id(driver_set['driver_trans_id']))\n",
    "    driver_date_column = 'driver_' + driver_date_column\n",
    "    driver_monthly_column = 'driver_' + driver_monthly_column\n",
    "\n",
    "    filter_expr = f\"{driver_monthly_column}='{monthly}'\"\n",
    "    print(f'filter expr', filter_expr)\n",
    "    driver_set = driver_set.filter(filter_expr)\n",
    "    print('filtered rec num:')\n",
    "    driver_set.groupBy(driver_date_column).count().sort(col(driver_date_column)).show(1000)\n",
    "\n",
    "    hdfs = HdfsManager(spark)\n",
    "    hdfs.delete(madmen_gcs_dir + \"_parquet\")\n",
    "\n",
    "    print(f\"{driver_date_column} from {madmen_time_range}\")\n",
    "    final_df = loadByDriver(\n",
    "        spark,\n",
    "        driver_set,\n",
    "        checkpoint=checkpoint,\n",
    "        time=madmen_time_range,\n",
    "        variables=var_list,\n",
    "        dateColumn=driver_date_column,\n",
    "        driverKeys=['driver_trans_id_clean'],\n",
    "        madmenKeys=['transaction'],\n",
    "        outputPath=madmen_gcs_dir + \"_parquet\",\n",
    "        groupSize=int(group_size),\n",
    "    )\n",
    "    print(f'pulled variable rec num:')\n",
    "    final_df.groupBy(driver_date_column).count().sort(col(driver_date_column)).show(100)\n",
    "\n",
    "    hdfs.delete(madmen_gcs_dir)\n",
    "    save_pig(spark, final_df, madmen_gcs_dir, \"\\x07\")\n",
    "\n",
    "    hdfs.delete(madmen_gcs_dir + \"_parquet\")\n",
    "\n",
    "    run_seconds = (datetime.now() - start_ts).total_seconds()\n",
    "    run_time_expr = f\"job {run_seconds / 60:.2f} minutes to finish\"\n",
    "    send_email(email_to, \n",
    "               subject=f\"[{madmen_time_range}] data pull routine job done\",\n",
    "               content=f\"data saved to {madmen_gcs_dir}, {run_time_expr}\",\n",
    "              )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcca71c-bc3c-4e02-b258-2ea99dff5678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755f428-b56a-4676-b25b-bc0b83d95c58",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pytz\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import aml.cloud_v1 as cloud\n",
    "from model_automation.gcp import dataproc_config\n",
    "\n",
    "\n",
    "model_name = 'guxia_ucc23'\n",
    "\n",
    "gcp_project_id = 'ccg24-hrzana-gds-focus'\n",
    "\n",
    "# run_dt = datetime.now(pytz.utc) + relativedelta(hours=8)\n",
    "run_dt = datetime(2023, 11, 3)\n",
    "\n",
    "driver_gcs_dir = 'gs://pypl-bkt-rsh-row-std-gds-focus/user/guxia/UCC23_V2/data/driver/20231101/20220401_20230731'\n",
    "madmen_gcs_dir = f\"gs://pypl-bkt-rsh-row-std-gds-focus/user/guxia/UCC23_V2/data/madmen/{run_dt.strftime('%Y%m%d')}\"\n",
    "\n",
    "print(madmen_gcs_dir)\n",
    "\n",
    "driver_start_dt = datetime(2022, 4, 1)\n",
    "driver_end_dt = datetime(2022, 6, 30)\n",
    "dates = pd.date_range(start=driver_start_dt, end=driver_end_dt, freq='1M')\n",
    "\n",
    "all_var = [\n",
    "    'fd_super_cookie',\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf85d2-6ab0-4981-83ae-d894a941e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_params = []\n",
    "for d in dates[:2]:\n",
    "    job_start_dt = d + relativedelta(day=1)\n",
    "    job_end_dt = d + relativedelta(days=1, seconds=-1)\n",
    "    monthly = d.strftime('%Y-%m')\n",
    "    madmen_time_range = f\"{job_start_dt.strftime('%Y-%m-%d %H:%M:%S')} to {job_end_dt.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "    print(monthly, madmen_time_range)\n",
    "\n",
    "    # job params\n",
    "    params = {\n",
    "        'var_list': all_var[:10],\n",
    "        'driver_date_column': 'pmt_start_date',\n",
    "        'driver_monthly_column': 'monthly',\n",
    "        'checkpoint': 'ConsolidatedFunding',\n",
    "        'madmen_time_range': madmen_time_range,\n",
    "        'email_to': 'guxia@paypal.com',\n",
    "        'driver_gcs_dir': driver_gcs_dir,\n",
    "        'madmen_gcs_dir': madmen_gcs_dir + f\"/{job_start_dt.strftime('%Y%m%d')}_{job_end_dt.strftime('%Y%m%d')}\",\n",
    "        'monthly': monthly,\n",
    "    }\n",
    "\n",
    "    print(json.dumps(params, indent=4))\n",
    "\n",
    "    job_params.append(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76cc79-18a1-458a-8f30-057014fdb0c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "gcp_client = cloud.DataProcClient(gcp_project=gcp_project_id)\n",
    "job_ids = []\n",
    "\n",
    "for params in job_params:\n",
    "    # submit job to dataproc\n",
    "    job_id = gcp_client.create_spark_job(\n",
    "    # function run on dataproc\n",
    "        func=pull_madmen_monthly,\n",
    "        packages_to_install=['automation_utils==0.3.0', 'gcsfs'],\n",
    "        wait_for_completion=False,\n",
    "    # function kwargs\n",
    "        params=params,\n",
    "    # dataproc config\n",
    "        **dataproc_config['medium'],\n",
    "    )\n",
    "    print(f'job created: {job_id}')\n",
    "    job_ids.append(job_id)\n",
    "\n",
    "for job_id in job_ids:\n",
    "    print('job id', job_id)\n",
    "    \n",
    "    # wait until job finished\n",
    "    status = gcp_client.wait_job_for_completion(job_id)\n",
    "    if status == \"FAILED\":\n",
    "        print(f\"job failed, job id: {job_id}\")\n",
    "\n",
    "    # save job log to local\n",
    "    log_name = job_id.split(os.sep)[-1]\n",
    "    os.makedirs('log', exist_ok=True)\n",
    "    with open(os.path.join('log', log_name), 'w') as f:\n",
    "        gcp_client.get_job_logs(job_id, file=f)\n",
    "\n",
    "    print(f'finish running data job')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e711eb7-bbab-4f0a-b48c-2a6f66531e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280fa358-40be-4b05-a93d-df59d4990e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1f126-c9b8-42b5-957c-2075db108246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e22c6f-bdc4-4712-976c-82d7f533338c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
