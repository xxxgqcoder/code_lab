{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305f6f09",
   "metadata": {},
   "source": [
    "# convert pandas dataframe to spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cdda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, FloatType\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "\n",
    "df = None # pandas dataframe object\n",
    "spark = None # spark instance\n",
    "\n",
    "# convert pandas dataframe to spark df\n",
    "schema = StructType([\n",
    "    StructField(\"monthly\", StringType(), True),\n",
    "    StructField(\"OP_perct\", StringType(), True),\n",
    "    StructField(\"score\", StringType(), True),\n",
    "    StructField(\"Score_Value\", DoubleType(), True),\n",
    "    StructField(\"bad_tag\", StringType(), True),\n",
    "    StructField(\"dim_name\", StringType(), True),\n",
    "    StructField(\"dim_value\", StringType(), True),\n",
    "    StructField(\"Catch_Rate\", DoubleType(), True),\n",
    "    StructField(\"Bad\", DoubleType(), True),\n",
    "    StructField(\"Good\", DoubleType(), True),\n",
    "    StructField(\"Total\", DoubleType(), True),\n",
    "    StructField(\"FPR\", DoubleType(), True),\n",
    "    StructField(\"wgt_alias\", StringType(), True),\n",
    "])\n",
    "\n",
    "spark_df = spark.createDataFrame(df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60bea25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "799c684e-d06c-4cbb-bb1b-5dda635422d5",
   "metadata": {},
   "source": [
    "# debug dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e2efc4-d249-40e4-ba3a-a59f263748aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [(\"2024-07-01\", \"2024-07\"),\n",
    "        (\"2024-09-01\", \"2024-09\"),\n",
    "        (\"2024-10-01\", \"2024-10\"),\n",
    "       ]\n",
    "\n",
    "columns = [\"driver_pmt_date\",\"driver_monthly\"]\n",
    "driver_df = spark.createDataFrame(data = data, schema = columns)\n",
    "driver_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa2dd50",
   "metadata": {},
   "source": [
    "# adding new column using case when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee64ee-85ae-4f61-a19f-f96b53e31ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ec5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "\n",
    "df = df.withColumn('is_declined_bad',\n",
    "                   when((col('decline_pattern').isNull()) | (col('decline_pattern') == lit('no pattern')), 0)\\\n",
    "                   .otherwise(1).cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdd7d3",
   "metadata": {},
   "source": [
    "# rename multi columns in select method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ce32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "\n",
    "join_keys = ['TRANS_ID']\n",
    "norm_postfix = '_normed'\n",
    "candidate_vars = ['feat1', 'feat2']\n",
    "\n",
    "ret = df.select(join_keys + [fn.col(c).alias(c + norm_postfix) for c in df.columns if c in candidate_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f523918e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dba390f",
   "metadata": {},
   "source": [
    "# spark data IO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1758828-382c-42fa-b042-71d349137d29",
   "metadata": {},
   "source": [
    "spark glob pattern\n",
    "- posix glob patternï¼š https://man7.org/linux/man-pages/man7/glob.7.html\n",
    "- hadoop glob pattern extension https://hail.is/docs/0.2/hadoop_glob_patterns.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece59ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parquet data with data path glob\n",
    "# 1\n",
    "spark.read.format('parquet').load('hdfs://path/to/data/all_vars_[0-9]*/group*')\n",
    "\n",
    "# 2\n",
    "spark.read.option(option_key, option_value).parquet('hdfs://path/to/data/all_vars_[0-9]*/group*')\n",
    "\n",
    "# read csv\n",
    "df = spark.read.options(delimiter='\\x07', header=True).csv(\"hdfs://path/to/directory/of/csv_file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f8012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9735051c",
   "metadata": {},
   "source": [
    "# write spark dataframe to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "df.write.mode('overwrite').parquet('hdfs://path/to/data')\n",
    "\n",
    "# 2\n",
    "df.write.option(option_key, option_vlaue).mode('overwrite').csv('hdfs://path/to/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ac010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8668d2b3",
   "metadata": {},
   "source": [
    "# column data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ecece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType,BooleanType,DateType\n",
    "\n",
    "for c in df.schema:\n",
    "    if isinstance(c.dataType, StringType):\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d6295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6eb79e",
   "metadata": {},
   "source": [
    "# trim string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f33400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, trim\n",
    "\n",
    "\n",
    "df = df.withColumn(column_name, trim(col(column_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdfb9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ffe91a",
   "metadata": {},
   "source": [
    "# union dataframe with different schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
    "\n",
    "df1.unionByName(df2, allowMissingColumns=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2df4af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a5b4c79",
   "metadata": {},
   "source": [
    "# dataframe groupby and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0264429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "df.groupBy('pmt_start_date').count().sort(col('pmt_start_date')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ba7b3",
   "metadata": {},
   "source": [
    "# coalesce dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad552b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345d960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a59a89ec",
   "metadata": {},
   "source": [
    "# parse date from integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "\n",
    "new_df = df.withColumn('event_date', \n",
    "                       fn.to_date(\n",
    "                           fn.from_unixtime(fn.col(\"time_event_published\").cast('string'), 'yyyy-MM-dd HH:mm:ss'), \n",
    "                           'yyyy-MM-sortdd')\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f17fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad9b9c9c",
   "metadata": {},
   "source": [
    "# load hive table into spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46755df6",
   "metadata": {},
   "source": [
    "config your notebook to access horton cluster\n",
    "```shell\n",
    "%url -c horton\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5198c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = None # spark instance\n",
    "\n",
    "df = spark.sql('select * from hive_db_name.hive_table_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111927a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46119d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce64d7-b066-4d39-8f5d-e7739dc9d441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f15643e9-618c-4a1b-8431-65fb4fffd9a5",
   "metadata": {},
   "source": [
    "# set column as null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7f56c-39ad-4396-9ac8-b8659dff0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "v = 'some_column_name'\n",
    "\n",
    "data_df = data_df.withColumn(f\"{v}_null\", lit(None).cast(StringType()))\\\n",
    "                .drop(v)\\\n",
    "                .withColumnRenamed(f\"{v}_null\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72af6c4-c3d5-4b65-9fcf-93944b15f779",
   "metadata": {},
   "source": [
    "# coalesce columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e9906-7d0f-47ec-959e-839130a21546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, trim, coalesce, udf\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def blank_as_null(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, str) and x.strip() == '':\n",
    "        return None\n",
    "    return x\n",
    "\n",
    "\n",
    "origin_df = origin_df.alias('origin_df')\n",
    "backfill_df = backfill_df.alias('backfill_df')\n",
    "\n",
    "ensembled_df = origin_df.join(backfill_df,\n",
    "                              on='driver_trans_id_clean',\n",
    "                              how='left'\n",
    "                             )\n",
    "\n",
    "origin_cols = set(origin_df.columns)\n",
    "backfill_vars = [c for c in backfill_df.columns if c not in ['driver_trans_id_clean']]\n",
    "for c in backfill_vars:\n",
    "    if c not in origin_cols:\n",
    "        print(f\"ignore column {c} since it's not in origin columns\")\n",
    "        continue\n",
    "        \n",
    "    ensembled_df = ensembled_df.withColumn(f\"{c}_\", coalesce(blank_as_null(f\"origin_df.{c}\"), f\"backfill_df.{c}\"))\\\n",
    "                        .drop(c)\\\n",
    "                        .withColumnRenamed(f\"{c}_\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595fa58-c784-4b7f-b652-ad4a4b8572a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f9b2cb1-246b-4099-bf76-21ae23b3b685",
   "metadata": {},
   "source": [
    "# apply UDF to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838acf05-ebee-4df3-8ba1-fb0bf1fce1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def clean_trans_id(trans_id):\n",
    "    return trans_id.split('.')[0]\n",
    "\n",
    "df = df.withColumn('driver_trans_id_clean', clean_trans_id(col('driver_trans_id')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9af44a-8c14-4340-98bb-92f714ea36f0",
   "metadata": {},
   "source": [
    "# check df join coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb35aed2-966f-48bc-b742-6f18f6c9568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "\n",
    "df1 = None\n",
    "df2 = None\n",
    "df1_join_key = 'trans_id_clean'\n",
    "df2_join_key = 'drvier_trans_id_clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee6fdd-32ad-4126-9dd8-0474ab73d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joined_df = df1.select([join_key, 'monthly']).join(df2.select([join_key]), on=df1.trans_id_clean == df2.driver_trans_id_clean, how='left') \n",
    "\n",
    "joined_df = joined_df.withColumn('unit', fn.lit(1))\n",
    "joined_df = joined_df.withColumn('has_join', fn.when(fn.col('driver_trans_id_clean').isNull(), 0).otherwise(1))\n",
    "\n",
    "\n",
    "stats = joined_df.groupBy(['monthly']).agg({'has_join': 'sum', 'unit': 'sum'}).sort('monthly')\n",
    "stats = stats.withColumnRenamed('sum(has_join)', 'has_join_sum')\n",
    "stats = stats.withColumnRenamed('sum(unit)', 'unit_sum')\n",
    "stats = stats.withColumn('join_coverage', fn.col('has_join_sum') / fn.col('unit_sum'))\n",
    "stats.printSchema()\n",
    "\n",
    "print('monthly join coverage')\n",
    "stats.show(100)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
