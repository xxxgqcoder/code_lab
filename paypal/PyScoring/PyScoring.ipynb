{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eece2e4d",
   "metadata": {},
   "source": [
    "# use pyScoring and UME to run predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82fb8c-2b62-474a-84e0-1fa978040e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = ''\n",
    "\n",
    "# set data dir that contains txn with variables\n",
    "data_dir = ''\n",
    "\n",
    "# to where you want to save scoring result\n",
    "score_data_save_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca640f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from automation_utils.spark.session import get_spark\n",
    "from py_dpu import load_pig, save_pig\n",
    "from PyGuanjia import GuanjiaClient\n",
    "from pyScoring import ModelScorer\n",
    "from pyScoring import UMEModel\n",
    "\n",
    "from credentials import GUANJIA_PASSWORD, PAZ_PASSWORD\n",
    "\n",
    "\n",
    "spark = get_spark(app_name='pyscoring_prediction',\n",
    "                  executor_instances=16,\n",
    "                  executor_instances_min=16,\n",
    "                  executor_instances_max=128,\n",
    "                  queue='risk_gds_focus',\n",
    "                  load_all_jars=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffce6a-4e0e-408f-bb74-37d3f385bfc1",
   "metadata": {},
   "source": [
    "- prepare UME file, either download from guanjia or exported locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1da0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "guanjia = GuanjiaClient(username=os.environ['USER'], isQA=False, isPAZ=True)\n",
    "guanjia.login(passwd=PAZ_PASSWORD)\n",
    "\n",
    "\n",
    "model_dir = 'local_model_dir'\n",
    "if os.path.exists(model_dir):\n",
    "    shutil.rmtree(model_dir)\n",
    "\n",
    "ret_model_dir = guanjia.download_model(modelId=model_id,\n",
    "                                       folderName=model_dir)\n",
    "\n",
    "print(f\"model ume downloaded to {ret_model_dir}\")\n",
    "\n",
    "model_path = [p for p in os.listdir(ret_model_dir) if '.m' in p]\n",
    "model_path = model_path[0]\n",
    "print(f\"model_path: {model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# data meta columns\n",
    "data_meta_cols = [] \n",
    "print(f'loading data from {data_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = ModelScorer(spark=spark)\n",
    "\n",
    "data_df = load_pig(spark, data_dir)\n",
    "\n",
    "ume = UMEModel(os.path.join(ret_model_dir, model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ce86a-ed7b-4e97-ba38-ff7ee5f8a892",
   "metadata": {},
   "source": [
    "- need specify col_names_for_default_outputs here, or pyscoring will generate one for you\n",
    "- can use multi ume model paths here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa34dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_df = scorer.create_score_df(input_df=data_df,\n",
    "                                    mfile_paths=[os.path.join(ret_model_dir, model_path)], \n",
    "                                    col_names_for_default_outputs=[ume.outputs[0]],\n",
    "                                   )\n",
    "\n",
    "# or keep all columns & score columns if you want\n",
    "scoring_df = scoring_df.select(data_meta_cols + ume.outputs)\n",
    "scoring_df.printSchema()\n",
    "\n",
    "save_pig(spark, scoring_df.coalesce(32), result_save_dir)\n",
    "print(f\"finish saving data to {result_save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e689d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac897a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fd89552",
   "metadata": {},
   "source": [
    "# VarLangExpressionBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88480ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_layer_score_name = 'clip_score'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyScoring import VarLangExpressionBuilder\n",
    "\n",
    "builder = VarLangExpressionBuilder(clip_layer_score_name,\n",
    "                                   f\"\"\"if ({numeric_layer_score_name} > 1000.0) 1000.0\n",
    "                                    else if ({numeric_layer_score_name} < 0.0) 0.0\n",
    "                                    else {numeric_layer_score_name}\"\"\", \n",
    "                                   {f'{numeric_layer_score_name}': DataType.DOUBLE.getId()})\n",
    "model.add_node(builder.build())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b72387",
   "metadata": {},
   "source": [
    "# Langbuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = LangBuilder()\n",
    "\n",
    "\n",
    "node2 =Â builder.build(\"\"\"\n",
    "                        main:\n",
    "                        (case\n",
    "                            (<\n",
    "                                ref:numeric_score\n",
    "                                con:0.0\n",
    "                            )\n",
    "                        \n",
    "                            con:0.0\n",
    "                            (>\n",
    "                                ref:numeric_score\n",
    "                                con:1000.0\n",
    "                            )\n",
    "                            con:1000.0\n",
    "                            ref:numeric_score\n",
    "                        )\n",
    "                        \"\"\", \n",
    "                        output='output_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74135768",
   "metadata": {},
   "source": [
    "# convert UME as graph node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68478de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyScoring import ModeltoNodeBuilder\n",
    "\n",
    "ume = None # UME model spec\n",
    "builder = ModeltoNodeBuilder(name='ems_score', \n",
    "                             computeLayers=ume.model.getComputeLayers(),\n",
    "                             resultMap={'model_score1': ume.outputs[0]}\n",
    "                            )\n",
    "node = builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb0e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecd27624-7af4-4998-b78f-f1264df04b93",
   "metadata": {},
   "source": [
    "# check node bining config in UME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b3496-5362-4bee-9158-78d34cd7d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ume_model.print_graph(level='all', node_names=['node_name_or_variable_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8762b2-0ccb-4f63-9d32-037503d34162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0685a37-909f-4a70-b390-e94a18a74673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4054d4c-1cee-41b0-be66-bfd143d800a0",
   "metadata": {},
   "source": [
    "# ensembled submodel and final model together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae1a32-4976-4d7f-863b-234b5a78ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume final model's input format is: {sub_model_name}_model_score1\n",
    "final_model_name = 'UCC21'\n",
    "\n",
    "sub_model_name = [\n",
    "    'UCC21_LOW_RISK',\n",
    "    'UCC21_UCC',\n",
    "    'UCC21_APPR',\n",
    "    'UCC21_OVERALL',\n",
    "]\n",
    "\n",
    "# prefix of each ume model directory. full directory will be f'{model_dir}_{model_name}.\n",
    "model_dir = 'model_dir'\n",
    "\n",
    "final_ume_save_dir = 'final_ume'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe7f71-d714-4760-a9e9-85bbb8141d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyScoring.graph import Graph\n",
    "from pyScoring import ModeltoNodeBuilder\n",
    "\n",
    "\n",
    "os.makedirs(final_ume_save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# mapping from model_name to UME\n",
    "model_umes = {}\n",
    "\n",
    "for model_name in [final_model_name] + sub_model_name:\n",
    "    ret_model_dir = f'{model_dir}_{model_name}'\n",
    "    model_ume_path =[f for f in os.listdir(ret_model_dir) if '.m' in f][0]\n",
    "    model_ume_path = os.path.join(ret_model_dir, model_ume_path)\n",
    "\n",
    "    print(f'loading ume from {model_ume_path}')\n",
    "    ume = UMEModel(model_ume_path)\n",
    "    model_umes[model_name] = ume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d72172b-d69f-4734-aabf-c0ccfa2a8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble nodes\n",
    "nodes = []\n",
    "outputs = []\n",
    "for model_name, ume in model_umes.items():\n",
    "    print(f'process model {model_name}')\n",
    "    if len(ume.outputs) > 1:\n",
    "        print(f'multi outputs detected in model {model_name}, will only take first one: {ume.outputs[0]}')\n",
    "    \n",
    "    builder = ModeltoNodeBuilder(name=f'{model_name}_intermediate',\n",
    "                                 computeLayers=ume.model.getComputeLayers(),\n",
    "                                 resultMap={f'{model_name}_model_score1': ume.outputs[0]}\n",
    "                                )\n",
    "    node = builder.build()\n",
    "    print(f'node name: {node.name}')\n",
    "\n",
    "    print(f'node output name: {node.outputs[0]}')\n",
    "    outputs.append(node.outputs[0])\n",
    "    nodes.append(node)\n",
    "    \n",
    "    \n",
    "# final UME\n",
    "graph = Graph()\n",
    "graph.add_nodes(nodes)\n",
    "final_ume = graph.generate_model_by_graph(model_name=f\"ensembled_{final_model_name}\",\n",
    "                                          model_outputs=outputs,\n",
    "                                          optimization=False)\n",
    "\n",
    "print(f'final model outputs', final_ume.outputs)\n",
    "final_ume.save(final_ume_save_dir)\n",
    "print(f'final model ume saved to {final_ume_save_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615403a9-cecc-4023-9d73-0f315240a9ec",
   "metadata": {},
   "source": [
    "# expose output of intermediate node in UME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e554b8a-8541-4f62-a1b5-0ebaca075a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ume.convert_to_graph()\n",
    "model_obj = graph.generate_model_by_graph(model_name='model_name_x', model_outputs=['Your_new_outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837c270-7e50-49ca-83df-0b6c64d750c9",
   "metadata": {},
   "source": [
    "# model score normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2749e27-7541-46e4-bea3-152a79f89a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "source score: UCC24\n",
    "\n",
    "target score: normalized target: normed UCC24 / UCC23_v2_normed_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b979a-d7e4-4b71-ba36-04df9a795641",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = None # pandas df which contains source score & target score\n",
    "project_map_save_path = '' # score projection save path\n",
    "\n",
    "# align ume output\n",
    "aligned_score_name = 'aligned_score'\n",
    "cliped_score_name = 'final_score'\n",
    "\n",
    "\n",
    "merged_ume_path = '' # original ume path\n",
    "\n",
    "bucket_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90233736-bdb9-4a01-b4cb-c4bf3a89c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_score_df: bin table: min_x , max_x\n",
    "\n",
    "target_score_df: bin table: min_y , max_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfb99b-1cf3-4ead-ae90-2de209451e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build score mapping\n",
    "pjmp = SAP.SAP_GEN_MAP(\n",
    "    input_score_df=score_df,\n",
    "    target_score_df=score_df,\n",
    "    binnumber=bucket_num,\n",
    "    input_score_name=ckpt,\n",
    "    target_score_name=target_score_col,\n",
    "    input_weight_name=cap_amt_col,\n",
    "    target_weight_name=cap_amt_col,\n",
    ")\n",
    "\n",
    "pjmp.to_csv(project_map_save_path, sep='\\x07', index=False)\n",
    "print(f'projection map saved to {project_map_save_path}')\n",
    "\n",
    "\n",
    "print('projection map')\n",
    "print(pjmp)\n",
    "print('=' * 120)\n",
    "\n",
    "\n",
    "print(f'loading origin UME from {merged_ume_path}')\n",
    "original_ume = UMEModel(merged_ume_path)\n",
    "\n",
    "original_ume_output_name = original_ume.outputs[0]\n",
    "print(f'original ume output name: {original_ume_output_name}')\n",
    "\n",
    "\n",
    "# map original score to target score, using pjmp\n",
    "aligned_ume =SAP.SAP_GEN_SPEC(\n",
    "    origin_model_spec=original_ume,\n",
    "    map_table=pjmp,\n",
    "    new_spec_name=f\"aligned_ume\",\n",
    "    new_layer_name='align_score_layer',\n",
    "    new_output_name=aligned_score_name,\n",
    "    orginal_output_position=0\n",
    ")\n",
    "\n",
    "\n",
    "print(f'aligned model ume outputs')\n",
    "print(aligned_ume.outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be26b5-cd6f-44c3-8830-19dfe62b765d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c00f33-a1a4-48fa-b8a7-abcad19793fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f3dc10-0c1c-4d22-a8e7-ddf25057b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm source score: binary search xmin\n",
    "\n",
    "def norm_by_pjmp_v2(df, source_score_col, normed_score_col, pjmp):\n",
    "    pjmp = pjmp.sort_values(by='xmin')\n",
    "    x_mins = pjmp['xmin']\n",
    "    min_x, min_y= min(pjmp[\"xmin\"]), min(pjmp[\"ymin\"])\n",
    "    max_x, max_y = max(pjmp[\"xmax\"]), max(pjmp[\"ymax\"])\n",
    "    \n",
    "    def proj(x):\n",
    "        if x <= min_x:\n",
    "            return min_y\n",
    "        \n",
    "        if x >= max_x:\n",
    "            return max_y\n",
    "\n",
    "        selected_chart = pjmp.loc[x_mins.searchsorted(x)-1]\n",
    "        Y_MAX = selected_chart[\"ymax\"]\n",
    "        Y_MIN = selected_chart[\"ymin\"]\n",
    "        X_MAX = selected_chart[\"xmax\"]\n",
    "        X_MIN = selected_chart[\"xmin\"]`\n",
    "        x_out = Y_MIN + ((Y_MAX - Y_MIN) * (x - X_MIN))/ (X_MAX - X_MIN)\n",
    "        return int(np.floor(min(1000.0, max(0.0,  x_out))))\n",
    "        \n",
    "    df[normed_score_col] = df[source_score_col].map(proj)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91a44f-329b-414e-b140-532b73eaf4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c37c05fc-4475-4cf7-a6ab-fdef94401a81",
   "metadata": {},
   "source": [
    "# UME prediction pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dec6ddc-492f-43ce-8c34-a16b2de44628",
   "metadata": {},
   "source": [
    "- when reading data with variables, do specify `dtype=str` ortherwise unexpected score result maybe found\n",
    "- remember to set keep_default_na as False, or empty string will replaced as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e91dc68-825e-43df-bc24-2a98e52cc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wzhao5_1700634531.csv', \n",
    "                 sep='\\x07', \n",
    "                 dtype=str, \n",
    "                 keep_default_na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da106242-df48-430d-9068-38d8fed5acd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c10a8a34-1777-4f5a-be05-670a2dca94bb",
   "metadata": {},
   "source": [
    "# package shifu norm layer & tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b366f4-19cf-4910-ac0a-6b6aa88a3d6a",
   "metadata": {},
   "source": [
    "- all shifu columns that are marked finalSelect=True will create a norm node. Mark column as finalSelect=False if you want to exclude it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8336c6-553b-4d61-a705-3d084635e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyScoring.shifu import ShifuTransformer\n",
    "from pyScoring.onnx.support.tf2.tf2_to_onnx import tf_model_to_onnx_as_spec\n",
    "from pyScoring import UMEModel, InputBuilder, NormalizeBuilder, ConstantBuilder, ReNameBuilder, ModeltoNodeBuilder, RegressionBuilder, NumericizeBuilder, LangBuilder, ClipBuilder\n",
    "\n",
    "\n",
    "shifu_mode_path = '' # path to shifu folder\n",
    "tf_export = None # loaded tensorflow model checkpoint\n",
    "tf_output_names = ['tf_score'] # output name for tensorflow model\n",
    "tf_inputs = ['var_name'] # variable names feed to tensorflow model\n",
    "model_ume_name = '' # packaged ume output name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a31e8b-5476-4f4f-8750-92a9166697d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifu_norm = ShifuTransformer(shifu_mode_path)\n",
    "norm_nodes = shifu_norm.create_shifu_transformation_nodes()\n",
    "\n",
    "tf_spec = tf_model_to_onnx_as_spec(tf_model=tf_export, output_mappings=tf_output_names, debug=False)\n",
    "\n",
    "input_names = []\n",
    "for x in tf_inputs:\n",
    "    input_names.append(x)\n",
    "input_nodes = InputBuilder(inputs=input_names, \n",
    "                           output=tf_spec.inputs[0]).build()\n",
    "\n",
    "\n",
    "merged_spec = merge_model_specs(specs=[tf_spec], \n",
    "                                extra_nodes=[input_nodes]+norm_nodes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ced35-6471-4522-8b0d-1adac7f0ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                model_outputs=tf_output_names,  \n",
    "                                model_name=model_ume_name+'_tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80dab6-f912-41b6-b885-727a12377d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7d2d9-b3fd-44ca-a7fc-8924dcd1eaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c6873-06ae-4705-a123-421ca993e9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e1f42-208d-402a-8178-a3f9beea7f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0df59-c313-4361-9b0e-b33c7af52c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28411ba9-fff2-4d5b-bab6-eb541703826e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
