{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea646e4-4e12-47ce-b65d-673527d507d6",
   "metadata": {},
   "source": [
    "# env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d937da-f3dd-428b-a491-cf5f0837f017",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "username = 'guxia'\n",
    "repo_dir = f'/projects/gds-focus/data/{username}/UCC_LATAM23/'\n",
    "secret_path = f'/projects/{username}/secret'\n",
    "os.chdir(repo_dir)\n",
    "for p in [secret_path, f\"{repo_dir}/utils\"]:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import aml.cloud_v1 as cloud\n",
    "cloud.notebook.authenticate_user()\n",
    "\n",
    "from automation_utils.common.file import read_by_line\n",
    "from model_automation.deep_learning_pipeline import data_prepare\n",
    "from model_automation.utils.rmr import run_cmd\n",
    "\n",
    "from MMoE import MMoE\n",
    "from decorators import timeit\n",
    "\n",
    "%reload_ext cloudmagics.bigquery\n",
    "%config PPMagics.domain=\"ccg24-hrzana-gds-focus\"\n",
    "%config PPMagics.autolimit=0\n",
    "%url -c horton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd03c94-2b03-4922-a49d-c25825f35093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb6b86-3c2d-4f7e-a6db-d91890130b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f680dd-cd6b-4c2b-8a82-77ec35933e96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e16bcd-446a-4664-8af8-fce8da4016f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_dir = os.getcwd()\n",
    "tfrecord_data_dir = \"data/tfrecord\"\n",
    "\n",
    "dev_data = \"UCC_LATAM_DEV_MULTI_SEG_0330_stc_cc\"\n",
    "oot_data = \"UCC_LATAM_OOT_MULTI_SEG_0330_stc_cc\"\n",
    "\n",
    "ucc_meta_cols_path = f\"{working_dir}/assets/ucc_meta_columns.txt\"\n",
    "candidate_vars_path = f\"{working_dir}/assets/candidate_variables.txt\"\n",
    "categorical_vars_path = f\"{working_dir}/assets/categorical_variables.txt\"\n",
    "\n",
    "weight_columns = [\"driver_dol_wgt\"]\n",
    "target_columns = [\"driver_is_cc_bad\"]\n",
    "\n",
    "model_ckpt_dir = os.path.join(working_dir, \"model_ckpt\")\n",
    "model_eval_dir = os.path.join(working_dir, 'model_eval')\n",
    "model_asset_dir = os.path.join(working_dir, 'model_asset')\n",
    "model_log_dir = os.path.join(working_dir, 'model_log')\n",
    "\n",
    "\n",
    "# segments and features\n",
    "shifu_models = [\n",
    "    \"varsel_ucc_latam_seg0\",    # overall\n",
    "    \"varsel_ucc_latam_seg1\",    # usdamt_500\n",
    "    \"varsel_ucc_latam_seg2\",    # usdamt_200\n",
    "    \"varsel_ucc_latam_seg3\",    # usdamt_100\n",
    "    \"varsel_ucc_latam_seg4\",    # iscbp_1, without stc/cc\n",
    "    \"varsel_ucc_latam_seg5\",    # ucc21_400\n",
    "    \"varsel_ucc_latam_seg6\",    # ccbinbadrt_0.008\n",
    "    \"varsel_ucc_latam_seg7\",    # iscbp_1, with stc/cc\n",
    "]\n",
    "\n",
    "stc_cc_vars = \"\"\"\n",
    "stc_pp_addr_line_match_score\n",
    "stc_pp_addr_zip_match_score\n",
    "stc_pp_addr_city_match_score\n",
    "stc_pp_addr_state_match_score\n",
    "stc_pp_name_match_score\n",
    "stc_pp_email_match_score\n",
    "stc_pp_ip_match_score\n",
    "stc_customer_dof\n",
    "ucc_cc_engagement\n",
    "ucc_trust_variable\n",
    "ucc_trust_combine\n",
    "ucc_cc_segment_crime\n",
    "\"\"\".strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0696c-cfc1-4c82-a059-d1c0874c985a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49c509d1-fb69-4182-8f24-e21f41f35fd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T07:21:33.768624Z",
     "iopub.status.busy": "2023-06-30T07:21:33.768314Z",
     "iopub.status.idle": "2023-06-30T07:21:33.772073Z",
     "shell.execute_reply": "2023-06-30T07:21:33.771509Z",
     "shell.execute_reply.started": "2023-06-30T07:21:33.768596Z"
    },
    "tags": []
   },
   "source": [
    "# hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b832d5-197e-41ef-83c3-862fa85a474c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'mmoe_debug'\n",
    "\n",
    "multi_seg_name = [\"overall\", \"usdamt_100\", \"ucc21_400\", \"bad_cc\", \"iscbp\",]\n",
    "seg_feat_num = []\n",
    "seg_idx = [0,3,5,6,7]\n",
    "\n",
    "batch_size = 1024\n",
    "dropout = 0.5\n",
    "learning_rate = 5e-4\n",
    "\n",
    "expert_structure = [128, 128]\n",
    "gate_structure = [64]\n",
    "\n",
    "overall_top = 550\n",
    "seg_top = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6156f-55bc-4722-84f5-ccacc3fc78da",
   "metadata": {},
   "source": [
    "# column preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45ac05-24cc-49b8-a9e7-6056bd2b1036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(ucc_meta_cols_path) as f:\n",
    "    ucc_meta_columns = f.read().strip('\\n').split('\\n')\n",
    "    ucc_meta_columns += ['CcbinhmcWAXRtBadCntTxn90d30_03']\n",
    "    \n",
    "with open(categorical_vars_path) as f:\n",
    "    categorical_columns = f.read().strip('\\n').split('\\n')\n",
    "\n",
    "with open(candidate_vars_path) as f:\n",
    "    candidate_columns = f.read().strip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879ed42-6ccd-4383-95d8-b18eae6f8036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fdeac0-34f2-438d-83cc-279ba07467b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load clean features\n",
    "clean_features_file = \"{}/assets/clean_features_{}.txt\".format(working_dir, datetime.today().strftime(\"%Y%m%d\"))\n",
    "\n",
    "if os.path.exists(clean_features_file):\n",
    "    print(f\"load clean features from {clean_features_file}\")\n",
    "    clean_features = read_by_line(clean_features_file)\n",
    "\n",
    "else:\n",
    "    print(f\"{clean_features_file} does not exist, load from SolCat\")\n",
    "    from PySolCat import SolCatOnlineVarLoader\n",
    "\n",
    "    onlineVarLoader = SolCatOnlineVarLoader('PAZ')\n",
    "    available_vars = onlineVarLoader.get_online_var_by_cp_status(checkpoint='ConsolidatedFunding', status=['audit clean', 'implemented'])\n",
    "    clean_features = [var.__dict__['_variable_name'] for var in available_vars]\n",
    "\n",
    "    with open(clean_features_file, 'w') as f:\n",
    "        f.write(\"\\n\".join(clean_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e6fb9-f63c-46e6-9f85-9c05dc8388b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f254bfc-d66d-4a31-80b7-18387cde0833",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load selected features\n",
    "seg_dfs = []\n",
    "\n",
    "for model in shifu_models:\n",
    "    if os.path.exists(f\"shifu_model/{model}/ColumnConfig_export_bkp.json\"):\n",
    "        with open(f\"shifu_model/{model}/ColumnConfig_export_bkp.json\") as f:\n",
    "            cfg = json.load(f)\n",
    "    else:\n",
    "        with open(f\"shifu_model/{model}/ColumnConfig.json\") as f:\n",
    "            cfg = json.load(f)\n",
    "\n",
    "    df_cfg = pd.DataFrame(cfg)\n",
    "    df_cfg['iv'] = df_cfg['columnStats'].map(lambda x: x['iv'])\n",
    "    df_cfg['missingPercentage'] = df_cfg['columnStats'].map(lambda x: x['missingPercentage'])\n",
    "\n",
    "    df_se = pd.read_csv(f\"shifu_model/{model}/varsel/se.0\", sep='\\t', names=['No.', 'Name', 'Mean', 'RMS', 'Variance'])\n",
    "\n",
    "    df = df_se.merge(right=df_cfg[['columnName', 'iv', 'missingPercentage', 'finalSelect']], how='left', left_on='Name', right_on='columnName')\n",
    "\n",
    "    df['isClean'] = df['Name'].isin(clean_features)\n",
    "\n",
    "    df_sel = df[df['finalSelect']]\n",
    "    df_sel = df_sel.reset_index().drop(columns=['index']).reset_index().rename(columns={'index': 'se_rank'})\n",
    "    df_sel['iv_rank'] = df_sel['iv'].rank(ascending=False)\n",
    "    df_sel['miss_rank'] = df_sel['missingPercentage'].rank()\n",
    "    df_sel = df_sel[df_sel['isClean']]\n",
    "    seg_dfs.append(df_sel)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111de4d2-bf43-4d54-8bb6-aee3c0e311b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b5379-ee09-4bb7-ada0-54d1d3299ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6bebd-de6e-45cb-80ff-3459c04be9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb75e01-43a4-4d6e-851a-858e9c4dd334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b95e0-1f94-4505-ac12-e969a7f8e7b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "assert max(seg_idx) < len(seg_dfs)\n",
    "\n",
    "for i in seg_idx:\n",
    "    if i == 7:\n",
    "        # specific to this notebook. cbp with(seg7)/without(seg4) STC/CC variables.\n",
    "        df = seg_dfs[4]\n",
    "    else:\n",
    "        df = seg_dfs[i]\n",
    "    \n",
    "    if i == 0:\n",
    "        se_top, iv_top = overall_top, overall_top\n",
    "    else:\n",
    "        se_top, iv_top = seg_top, seg_top\n",
    "\n",
    "    # top ranking SE & top ranking IV\n",
    "    var_set = set(df[df['se_rank']< se_top]['Name'].tolist()) | set(df[df['iv_rank'] < iv_top]['Name'].tolist())\n",
    "    if i == 7:\n",
    "        var_set = var_set | set(stc_cc_vars)\n",
    "    var_list = list(var_set)\n",
    "    if i > 0:\n",
    "        var_list = [x + f\"__seg{i}\" for x in var_list]\n",
    "    print(\"seg\", i, len(var_list))\n",
    "    feature_columns.extend(var_list)\n",
    "    \n",
    "# colunmn in Shifu ColumnConfig doesnt contain suffix. But when you export Shifu norm layer, you need to append suffix to\n",
    "# each output variables of norm layer.\n",
    "df = pd.DataFrame(feature_columns, columns=['col_name'])\n",
    "df['seg_idx'] = df['col_name'].map(lambda x: int(x.split(\"__seg\")[-1]) if \"__seg\" in x else 0)\n",
    "df.sort_values(by=['seg_idx', 'col_name'], ascending=True, inplace=True)\n",
    "\n",
    "seg_feat_num = df.groupby(['seg_idx']).size().tolist()\n",
    "feature_columns = df['col_name'].tolist()\n",
    "\n",
    "assert len(seg_feat_num) == len(multi_seg_name)\n",
    "print(len(feature_columns))\n",
    "print(seg_feat_num)\n",
    "\n",
    "# save features to file\n",
    "feature_save_path = os.path.join(model_asset_dir, model_name, 'features.txt')\n",
    "os.makedirs(os.path.dirname(feature_save_path), exist_ok=True)\n",
    "\n",
    "with open(feature_save_path, 'w') as f:\n",
    "    f.write(\"\\n\".join(feature_columns))\n",
    "print(f'feature columns written to {feature_save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc5f02-0f74-4050-ba33-5153323f20a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed961802-def0-4f76-9a5b-7e6ca61df15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature stats\n",
    "print(\"feature dim = \", len(feature_columns))\n",
    "var_names = set()\n",
    "for var in feature_columns:\n",
    "    var_names.add(var.split(\"__seg\")[0])\n",
    "print(\"distinct feature name\", len(var_names))\n",
    "\n",
    "\n",
    "for c in set(weight_columns + target_columns):\n",
    "    if c not in ucc_meta_columns:\n",
    "        continue\n",
    "    ucc_meta_columns.remove(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed728178-1bd3-4d92-9d03-ae26da140f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b996800-d84c-4ed4-8b7d-e8e872288848",
   "metadata": {},
   "source": [
    "# data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8aa417-c644-4ace-ba9d-ed661000eea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150609d-cd06-4d05-ae41-b6c1633a8d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def filter_other_bad(feat, target, wgt):\n",
    "    if target >= 2.0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13901603-fd58-4bd4-9e7f-1282c1c1fd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update_target_vec(feat, target, wgt):\n",
    "    is_bad = tf.cast(target, tf.float32)\n",
    "    is_good = tf.math.subtract(1.0, is_bad)\n",
    "    log1p_usd_amt = tf.math.log1p(wgt)\n",
    "    bad_regression = tf.math.subtract(tf.math.multiply_no_nan(is_bad, log1p_usd_amt), tf.math.multiply_no_nan(is_good, log1p_usd_amt))\n",
    "    # for target column, use tuple if multiple target, otherwise tf will fail to calculate loss\n",
    "    new_target = (is_bad, bad_regression)\n",
    "    return (feat, new_target, wgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d94071-dc60-4408-af85-f9d4be52982d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update_wgt(feat, target, wgt):\n",
    "    log1p_usd_amt = tf.math.log1p(wgt)\n",
    "    return (feat, target, log1p_usd_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc49f3-ca6c-405a-9780-31826edd7ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def filter_oot(fvec, target, wgt, meta):\n",
    "    monthly = meta[38]\n",
    "    return tf.math.logical_or(tf.math.equal(monthly, '2022-11-01'),\n",
    "                              tf.math.logical_or(tf.math.equal(monthly, '2022-12-01'),\n",
    "                                                 tf.math.equal(monthly, '2023-01-01')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ed61c-ac69-426a-8507-6771d1da016b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepare = data_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0ea58-31a7-41e6-a6d5-bf675ea9a505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEV\n",
    "devdata_file_list = prepare.get_filenames(os.path.join(tfrecord_data_dir, dev_data))\n",
    "\n",
    "dev_dataset = prepare.make_dataset(\n",
    "    filenames=devdata_file_list,\n",
    "    feature_list=feature_columns,\n",
    "    target_list=target_columns,\n",
    "    weight_list=weight_columns,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9447e626-6f26-4631-8924-87ee92893f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OOT\n",
    "ootdata_file_list = prepare.get_filenames(os.path.join(tfrecord_data_dir, oot_data))\n",
    "\n",
    "oot_dataset = prepare.make_dataset(\n",
    "    filenames=ootdata_file_list,\n",
    "    feature_list=feature_columns,\n",
    "    target_list=target_columns,\n",
    "    weight_list=weight_columns,\n",
    "    meta_list=ucc_meta_columns,\n",
    "    batch_size=batch_size * 10,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361590b-3edf-4b0f-98d7-9db2c849c15d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058630b-9d6e-499d-8355-94801be60dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b3776-4028-417b-975e-89f901edeb50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process and cache dataset\n",
    "\n",
    "filtered_oot_dataset = oot_dataset.unbatch().filter(filter_oot).batch(batch_size*20)\n",
    "\n",
    "processed_dev_dataset = dev_dataset.unbatch()\\\n",
    "                                    .filter(filter_other_bad)\\\n",
    "                                    .map(update_target_vec)\\\n",
    "                                    .map(update_wgt)\\\n",
    "                                    .batch(batch_size)\n",
    "\n",
    "\n",
    "# all transformation made to dataset must happend before cache api call. Once loading dataset from\n",
    "# cahced data, you cannnot make any transformation to it any more.\n",
    "tf_cache_dir = os.path.join(working_dir, 'tf_cache_dir', f'{model_name}_oot')\n",
    "print(f\"caching dataset to {tf_cache_dir}\")\n",
    "run_cmd(f\"rm -r {tf_cache_dir}\")\n",
    "filtered_oot_dataset = filtered_oot_dataset.cache(tf_cache_dir)\n",
    "\n",
    "tf_cache_dir = os.path.join(working_dir, 'tf_cache_dir', f'{model_name}_dev')\n",
    "print(f\"caching dataset to {tf_cache_dir}\")\n",
    "run_cmd(f\"rm -r {tf_cache_dir}\")\n",
    "processed_dev_dataset = processed_dev_dataset.cache(tf_cache_dir).shuffle(batch_size * 100, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802f8c6-2671-439f-8c3e-be99857b6795",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c828cd-ee16-4af2-b3d3-d9e8218bf122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00028e7a-c1f2-4e16-a8b2-22f0260b599b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34246fc4-74c2-45a2-a030-6b74a9b0484c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e85633-7229-4f77-847b-cd35fe990255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10c38b-c6f6-48ff-98be-b1838adf5932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1018cb5-086a-41dc-9c97-100ca1a054ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd339ea-4dec-430a-82b3-53a4272c7290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c805b03a-4496-4b13-ba64-eef4a857e3b2",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d333a-ae74-4156-b4e4-4b4f5e476677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@timeit\n",
    "def model_prediction(models, ds, meta_list):\n",
    "    \"\"\"\n",
    "    run prediction for models.\n",
    "    models: list of tuple, first element is model name, second element is tf model\n",
    "    \"\"\"\n",
    "    final_df = pd.DataFrame()\n",
    "    batch_cnt = 0\n",
    "    for r in ds:\n",
    "        batch_cnt += 1\n",
    "        if batch_cnt % 100 == 0:\n",
    "            print(batch_cnt)\n",
    "        scores = []\n",
    "        for _, model in models:\n",
    "            score = model.predict(r[0])[0] # model specific, we produce two outputs and first one is is_cc_bad binary classification output\n",
    "            scores.append(score)\n",
    "        metas = r[-1].numpy()\n",
    "        target_vec = r[1]\n",
    "        data = np.concatenate([metas] + [target_vec] + scores, axis=1)\n",
    "        df = pd.DataFrame(data, columns=meta_list + ['driver_is_cc_bad'] + [f'{name}_score' for name, _ in models])\n",
    "        for c in meta_list:\n",
    "            df[c] = df[c].str.decode('UTF-8')\n",
    "        final_df = pd.concat([final_df, df])\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a8951-a5e5-43fe-ac61-e047641f6927",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def append_segments(df):\n",
    "    def _to_float(x, default_val='0'):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except:\n",
    "            return default_val\n",
    "\n",
    "    # apepnd segments flag\n",
    "    df['asp_1000'] = df['driver_capture_usd_amt'].map(lambda x: 1 if _to_float(x, 0) >= 1000 else 0)\n",
    "    df['is_oot'] = df['driver_monthly'].map(lambda x: 1 if x >= '2022-11-01' else 0)\n",
    "    # cap amt\n",
    "    df['dol_cap_1000'] = df['driver_capture_usd_amt'].map(lambda x: 1000.0 if _to_float(x, 0) >= 1000.0 else x)\n",
    "    \n",
    "    print(f'eval result record num: {df.shape[0]}')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b9c728-2a10-4732-ad3e-47833e7e1e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MMoE(\n",
    "    expert_dim=seg_feat_num,\n",
    "    expert_names=multi_seg_name,\n",
    "    expert_structure=expert_structure,\n",
    "    expert_activation=tf.nn.relu,\n",
    "    gate_structure=gate_structure,\n",
    "    gate_activation=tf.nn.relu,\n",
    "    task_num=2,\n",
    "    task_names=['cc_bad', 'loss_reg'],\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss={\n",
    "        'Task_cc_bad': 'binary_crossentropy',\n",
    "        'Task_loss_reg': 'mse',\n",
    "    },\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    metrics={\n",
    "        'Task_cc_bad': ['accuracy', tf.keras.metrics.AUC()],\n",
    "        'Task_loss_reg': 'mse'\n",
    "    },\n",
    "    # , loss_weight=[1,0.5]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a992d2-71bd-4651-81c4-35d0b12dc3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7dae2-7b2c-43bd-9b25-28fad7767cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874600c-2b1f-46d1-b149-1e7ad5e3a035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f434b-9f28-4b9d-8900-7de2d1ec2b89",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epoch_num = 10\n",
    "ckdir = os.path.join(model_ckpt_dir, model_name)\n",
    "initial_epoch = 0\n",
    "\n",
    "os.makedirs(os.path.join(model_ckpt_dir, model_name), exist_ok=True)\n",
    "\n",
    "# training process\n",
    "print(f'begin model training')\n",
    "while (not os.path.exists(ckdir)) or (len(os.listdir(ckdir)) < epoch_num):\n",
    "    ck_num = len(os.listdir(ckdir))\n",
    "    print(f'existing ck num: {ck_num}')\n",
    "    initial_epoch = ck_num\n",
    "    if ck_num > 0:\n",
    "        ckpt_file = sorted(os.listdir(ckdir))[-1]\n",
    "        print(f'loading from {ckpt_file}')\n",
    "        model = tf.keras.models.load_model(os.path.join(ckdir, ckpt_file))\n",
    "\n",
    "    ck_filepath = os.path.join(ckdir, \"model-{epoch:02d}.ckpt\")\n",
    "    ck_callback = ModelCheckpoint(\n",
    "        ck_filepath,\n",
    "        monitor='loss',\n",
    "        verbose=1,\n",
    "        save_best_only=False,\n",
    "        mode='min',\n",
    "        save_weights_only=False,\n",
    "        save_freq='epoch',\n",
    "    )\n",
    "\n",
    "    log_dir = os.path.join(model_log_dir, model_name)\n",
    "    model_log_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        processed_dev_dataset,\n",
    "        use_multiprocessing=True,\n",
    "        workers=32,\n",
    "        initial_epoch=initial_epoch,\n",
    "        epochs=epoch_num,\n",
    "        verbose=1,\n",
    "        max_queue_size=64,\n",
    "        shuffle=True,\n",
    "        callbacks=[ck_callback, model_log_callback],\n",
    "    )\n",
    "\n",
    "    print(history.history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6a955-fc3c-4f79-aa90-1406d01d78a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f8fc0-1250-4c69-b5b0-24d1d571f304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# run prediction\n",
    "print(f'begin model prediction')\n",
    "os.makedirs(os.path.join(model_eval_dir, model_name), exist_ok=True)\n",
    "model_files = os.listdir(os.path.join(model_ckpt_dir, model_name))\n",
    "\n",
    "ckpt_models = []\n",
    "for f in model_files:\n",
    "    print(f'loading {f}')\n",
    "    m = re.search(r'(\\d+)', f)\n",
    "    if not m:\n",
    "        raise ValueError(f'cannot find epoch in {f}')\n",
    "    ck = m[1]\n",
    "    name = f'{model_name}_ck{ck}'\n",
    "    print(f'adding model {name}')\n",
    "    ckpt_model = tf.keras.models.load_model(os.path.join(model_ckpt_dir, model_name, f))\n",
    "    ckpt_models.append((name, ckpt_model))\n",
    "\n",
    "eval_df = model_prediction(ckpt_models,\n",
    "                           filtered_oot_dataset,\n",
    "                           ucc_meta_columns)\n",
    "\n",
    "eval_df = append_segments(eval_df)\n",
    "\n",
    "eval_df = eval_df[eval_df.is_oot == 1]\n",
    "\n",
    "eval_result_path = os.path.join(model_eval_dir, model_name, f'eval_data.csv')\n",
    "eval_df.to_csv(eval_result_path, index=False)\n",
    "print(f\"model prediction result saved to {eval_result_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe42cf5-d4dd-489f-8abd-ebd9e2ef59fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463aea81-097b-451f-b646-fcb62137b617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc111fb5-89be-4b14-ace3-4a9c5da9e2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a1818c7-43c9-40b1-9601-731db9ddcee7",
   "metadata": {},
   "source": [
    "# model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406606a5-95ed-44b7-b11f-62ff9f7e6116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = f'/projects/gds-focus/data/wzhao5/Python3'\n",
    "if p not in sys.path:\n",
    "    sys.path.append(p)\n",
    "\n",
    "\n",
    "predict_data_path = os.path.join(model_eval_dir, model_name, f'eval_data.csv')\n",
    "print(f'prediction data path: {predict_data_path}')\n",
    "\n",
    "df = pd.read_csv(predict_data_path, nrows=10)\n",
    "score_names = [c for c in df.columns if model_name in c]\n",
    "print('loaded score names: \\n{}'.format('\\n'.join(score_names)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90cafc-2622-4720-856a-5dad0c934a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "from fast_perf_v5 import fast_perf_v5\n",
    "\n",
    "args = {\n",
    "    'dataPath': predict_data_path,\n",
    "    'delimiter': ',',\n",
    "    'badList': ['driver_is_cc_bad'],\n",
    "    'scoreList': score_names,\n",
    "    'dimList': [\n",
    "        'driver_monthly',\n",
    "        'driver_monthly*driver_is_cbp',\n",
    "    ],\n",
    "    \n",
    "    'xWeight': ['dol_cap_1000'],\n",
    "    'yWeight': ['dol_cap_1000'],\n",
    "    'weightAlias': ['dol_cap_1000'],\n",
    "    'filterExpr': {\n",
    "        '$AND': {\n",
    "            'driver_decline_type': 'Approved',\n",
    "            'driver_capture_usd_amt': {'$notnull': ''},\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    'OP' : 100,\n",
    "    'TopOP': 20,\n",
    "    'outputFile': os.path.join(model_eval_dir, model_name, f'{model_name}_gainchart.xlsx'),\n",
    "}\n",
    "\n",
    "perf = fast_perf_v5(**args)\n",
    "perf.run()\n",
    "\n",
    "df_out = perf.output.copy()\n",
    "df_out.to_csv(os.path.join(model_eval_dir, model_name, f'{model_name}_gainchart.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a46bb-9db5-4fc1-ad58-defbefa96f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b1f01-e09d-455a-9e67-accf27ff5db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gain_chart_data_path = os.path.join(model_eval_dir, model_name, f'{model_name}_gainchart.csv')\n",
    "gain_chart = pd.read_csv(gain_chart_data_path)\n",
    "\n",
    "ret = pd.pivot_table(gain_chart[(gain_chart['Metric'] == 'dol_cap_1000 Catch_Rate') \n",
    "                                & (gain_chart['dim_name'] == 'driver_monthly')\n",
    "                               ],\n",
    "                     columns='dim_value',\n",
    "                     index=['score'],\n",
    "                     values=['OP4'])\n",
    "\n",
    "ret['avg'] = ret.mean(axis=1)\n",
    "\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1f736-ee74-43b5-ac78-dcd145ce9a32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ret = pd.pivot_table(gain_chart[(gain_chart['Metric'] == 'dol_cap_1000 Catch_Rate') \n",
    "                                & (gain_chart['dim_name'] == 'driver_monthly*driver_is_cbp')\n",
    "                                & (gain_chart['dim_2'] == 1)\n",
    "                               ],\n",
    "                     columns='dim_1',\n",
    "                     index=['score'],\n",
    "                     values=['OP4'])\n",
    "\n",
    "ret['avg'] = ret.mean(axis=1)\n",
    "\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50f800-8742-4fa0-99ee-8045dcf4329f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d495e-bb3f-4f42-a634-5a72d0e1af80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
